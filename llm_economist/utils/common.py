from enum import Enum
from collections import Counter
import random
import os
import numpy as np
try:
    from scipy import stats
except (ImportError, ValueError):  # pragma: no cover - lightweight fallback
    class _StatsFallback:
        class f:
            @staticmethod
            def ppf(p, d1, d2):
                arr = np.asarray(p, dtype=float)
                if arr.shape == ():
                    arr = np.array([arr], dtype=float)
                return np.ones_like(arr, dtype=float)

        @staticmethod
        def gaussian_kde(data):
            class _KDE:
                def __init__(self, payload):
                    self.payload = np.asarray(payload, dtype=float)

                def __call__(self, x):
                    return np.ones(1, dtype=float)

            return _KDE(data)

    stats = _StatsFallback()
import pandas as pd
from typing import List, Tuple

KEY = os.getenv('ECON_OPENAI')

class Message(Enum):
    SYSTEM = 1
    UPDATE = 2
    ACTION = 3

# Shared state for persona messages
GEN_ROLE_MESSAGES = {}

def labor_list(num_agents):
    base_value = 50
    offset = 10
    
    # Start value decreases as the number of agents increases
    start_value = base_value - (num_agents - 1) * offset // 2
    
    # Generate the list
    return [abs(start_value + i * offset) % 100 for i in range(num_agents)]

def count_votes(votes_list: list):
    # Count votes for each candidate
    max_count = max(votes_list.count(vote) for vote in set(votes_list))
    
    # Find all candidates with the maximum count
    tied_candidates = [vote for vote in set(votes_list) if votes_list.count(vote) == max_count]
    
    # Randomly select a candidate from the tied candidates, winner
    elected_tax_planner = random.choice(tied_candidates)
    
    # Extract the integer index from the winner's name
    #elected_tax_planner = int(winner.split("_")[-1])
    
    return elected_tax_planner

def distribute_agents(num_agents, agent_mix):
    # Calculate the approximate number of agents in each group
    adversarial_agents = round(agent_mix[2] / 100 * num_agents)
    selfless_agents = round(agent_mix[1] / 100 * num_agents)
    greedy_agents = num_agents - adversarial_agents - selfless_agents  # Remaining agents go to greedy

    # Return a list of agent types
    agents = ['adversarial'] * adversarial_agents + ['altruistic'] * selfless_agents + ['egotistical'] * greedy_agents

    # Shuffle the list to randomize agent assignments
    random.shuffle(agents)

    return agents

# Following R source code from GAMLSS package
# Default is U.S. Income distribution from ACS 2023
def qGB2(p, mu=72402.78177917618, sigma=2.0721070746154746, nu=0.48651871959386955, tau=1.1410398548220329, lower_tail=True, log_p=False):
    """
    Quantile function for the Generalized Beta of the Second Kind (GB2) distribution.
    
    Parameters:
    -----------
    p : float or array-like
        Probabilities
    mu : float
        Scale parameter (must be positive)
    sigma : float
        Shape parameter
    nu : float
        Shape parameter (must be positive)
    tau : float
        Shape parameter (must be positive)
    lower_tail : bool
        If True, probabilities are P[X ≤ x], otherwise P[X > x]
    log_p : bool
        If True, probabilities are given as log(p)
    
    Returns:
    --------
    q : float or array-like
        Quantiles corresponding to the probabilities in p
    """
    # Parameter validation
    if np.any(mu <= 0):
        raise ValueError("mu must be positive")
    if np.any(nu <= 0):
        raise ValueError("nu must be positive")
    if np.any(tau <= 0):
        raise ValueError("tau must be positive")
    
    # Handle log probabilities if needed
    if log_p:
        p = np.exp(p)
    
    # Validate probability range
    if np.any(p <= 0) or np.any(p >= 1):
        raise ValueError("p must be between 0 and 1")
    
    # Handle lower.tail parameter
    if not lower_tail:
        p = 1 - p
    
    # Handle sigma sign
    if hasattr(sigma, "__len__"):
        p = np.where(sigma < 0, 1 - p, p)
    else:
        if sigma < 0:
            p = 1 - p
    
    # Use F distribution's quantile function (ppf is the scipy equivalent of R's qf)
    w = stats.f.ppf(p, 2 * nu, 2 * tau)
    
    # Transform to GB2 quantiles
    q = mu * (((nu/tau) * w)**(1/sigma))
    
    return q

# Default is U.S. Income distribution from ACS 2023
def rGB2(n, mu=72402.78177917618, sigma=2.0721070746154746, nu=0.48651871959386955, tau=1.1410398548220329):
    """
    Generate random samples from the Generalized Beta of the Second Kind (GB2) distribution.
    
    Parameters:
    -----------
    n : int
        Number of random values to generate
    mu : float
        Scale parameter (must be positive)
    sigma : float
        Shape parameter
    nu : float
        Shape parameter (must be positive)
    tau : float
        Shape parameter (must be positive)
    
    Returns:
    --------
    r : array-like
        Random samples from the GB2 distribution
    """
    # Parameter validation
    if np.any(mu <= 0):
        raise ValueError("mu must be positive")
    if np.any(nu <= 0):
        raise ValueError("nu must be positive")
    if np.any(tau <= 0):
        raise ValueError("tau must be positive")
    
    # Ensure n is an integer
    n = int(np.ceil(n))
    
    # Generate uniform random numbers
    p = np.random.uniform(0, 1, size=n)
    
    # Transform using the quantile function
    r = qGB2(p, mu=mu, sigma=sigma, nu=nu, tau=tau)
    
    return r

def linear_transform(samples, old_min, old_max, new_min, new_max):
    """Linear transformation using NumPy for efficiency"""
    samples_array = np.array(samples)
    transformed = (samples_array - old_min) / (old_max - old_min) * (new_max - new_min) + new_min
    return transformed

def saez_optimal_tax_rates(skills, brackets, elasticities):
    """
    Calculate Saez optimal marginal tax rates for income brackets based on skills.
    
    Parameters:
    -----------
    skills : list of float
        List of individual skills (incomes/100).
    brackets : list of float
        List of income‐cutoff points [min1, min2, ..., max_value];
        each consecutive pair defines one bracket.
    elasticities : float or list of float
        If a single float: apply this elasticity to every bracket.
        If a list: must have length = (number of brackets), i.e. len(brackets)-1,
        giving one elasticity per bracket.
        
    Returns:
    --------
    tax_rates : list of float
        Optimal marginal tax rates for each bracket, in percentages
        (e.g., [12.88, 3.23, 3.23]).
    """
    # Convert skills to incomes
    incomes = np.array(skills) * 100.0
    brackets = np.array(brackets)
    
    # Build elasticity list
    n_brackets = len(brackets) - 1
    if isinstance(elasticities, (int, float)):
        elasticities = [float(elasticities)] * n_brackets
    else:
        if len(elasticities) != n_brackets:
            raise ValueError(f"elasticities must be length {n_brackets}, got {len(elasticities)}")
        elasticities = [float(e) for e in elasticities]
    
    # Sort incomes and compute welfare weights
    incomes = np.sort(incomes)
    welfare_weights = 1.0 / np.maximum(incomes, 1e-10)
    welfare_weights /= welfare_weights.sum()
    
    # Estimate density
    kde = stats.gaussian_kde(incomes)
    
    tax_rates = []
    for i in range(n_brackets):
        bracket_start, bracket_end = brackets[i], brackets[i+1]
        # choose z at midpoint (or near start for top bracket)
        if i < n_brackets - 1:
            z = 0.5 * (bracket_start + bracket_end)
        else:
            z = bracket_start + 0.1 * (bracket_end - bracket_start)
        
        F_z = np.mean(incomes <= z)
        f_z = kde(z)[0]
        
        # Pareto‐tail parameter a(z)
        if F_z < 1.0:
            a_z = (z * f_z) / (1.0 - F_z)
        else:
            a_z = 10.0
        
        # for the top bracket refine a(z)
        incomes_above = incomes[incomes >= z]
        if i == n_brackets - 1 and incomes_above.size > 0:
            m = incomes_above.mean()
            a_z = m / (m - bracket_start)
        
        # G(z): average welfare weight above z, normalized
        if incomes_above.size > 0 and F_z < 1.0:
            G_z = welfare_weights[incomes >= z].sum() / (1.0 - F_z)
        else:
            G_z = 0.0
        
        # pick the right elasticity for this bracket
        ε = elasticities[i]
        
        # Saez optimal rate τ = (1 - G) / [1 - G + a * ε]
        tau = (1.0 - G_z) / (1.0 - G_z + a_z * ε)
        tau = max(0.0, min(1.0, tau))
        
        tax_rates.append(round(tau * 100, 2))
    
    return tax_rates

def generate_synthetic_data(csv_path: str, n_samples: int) -> List[Tuple[str, str, int]]:
    """
    Generate synthetic data points following the distribution of occupations by sex by age.
    
    Args:
        csv_path: Path to the CSV file containing the distribution data
        n_samples: Number of synthetic data points to generate
    
    Returns:
        List of tuples, each containing (occupation, sex, age)
    """
    # Read the CSV file
    df = pd.read_csv(csv_path)
    
    # Create age category labels and their corresponding age ranges
    age_columns = ['Under 18', '18-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75+']
    age_ranges = {
        'Under 18': (14, 17),  # Assuming working age starts at 14
        '18-24': (18, 24),
        '25-34': (25, 34),
        '35-44': (35, 44),
        '45-54': (45, 54),
        '55-64': (55, 64),
        '65-74': (65, 74),
        '75+': (75, 90)  # Assuming max working age is 90
    }
    
    # Calculate total distribution
    total_distribution = df[age_columns].sum().sum()
    
    # Create a list to store the synthetic data
    synthetic_data = []
    
    for _ in range(n_samples):
        # Randomly select a row based on the distribution
        random_value = random.uniform(0, total_distribution)
        cumulative_sum = 0
        selected_row = None
        selected_age_column = None
        
        for idx, row in df.iterrows():
            for age_col in age_columns:
                cumulative_sum += row[age_col]
                if cumulative_sum >= random_value:
                    selected_row = row
                    selected_age_column = age_col
                    break
            if selected_row is not None:
                break
        
        # Get the selected occupation, sex, and generate a specific age within the range
        occupation = selected_row['Occupation_Label']
        sex = selected_row['SEX_Label']
        age_range = age_ranges[selected_age_column]
        specific_age = random.randint(age_range[0], age_range[1])
        
        # Add the synthetic data point
        synthetic_data.append((occupation, sex, specific_age))
    
    return synthetic_data
