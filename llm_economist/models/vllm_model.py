"""
vLLM model implementation for the LLM Economist framework.
"""

from typing import Tuple, Optional
import os
import requests
import json
from openai import OpenAI, RateLimitError
from time import sleep
from .base import BaseLLMModel


class VLLMModel(BaseLLMModel):
    """vLLM model implementation for local and remote vLLM deployments."""
    
    def __init__(self, model_name: str = "meta-llama/Llama-3.1-8B-Instruct", 
                 base_url: str = "http://localhost:8000", 
                 api_key: str = "economist",
                 max_tokens: int = 1000, temperature: float = 0.7):
        """
        Initialize the vLLM model.
        
        Args:
            model_name: Name of the model to use
            base_url: Base URL for the vLLM server
            api_key: API key for authentication
            max_tokens: Maximum number of tokens to generate
            temperature: Temperature for sampling
        """
        super().__init__(model_name, max_tokens, temperature)
        
        self.base_url = base_url
        self.api_key = api_key
        
        # Initialize OpenAI client for vLLM compatibility
        self.client = OpenAI(
            api_key=api_key,
            base_url=f"{base_url}/v1"
        )
        
        # Model name mapping for backward compatibility
        self.model_mapping = {
            'llama3:8b': 'meta-llama/Llama-3.1-8B-Instruct',
            'llama3:70b': 'meta-llama/Llama-3.1-70B-Instruct',
            'gemma3:27b': 'google/gemma-3-27b-it',
        }
        
        # Use mapped model name if available
        if model_name in self.model_mapping:
            self.model_name = self.model_mapping[model_name]
        
    def send_msg(self, system_prompt: str, user_prompt: str, 
                 temperature: Optional[float] = None, 
                 json_format: bool = False) -> Tuple[str, bool]:
        """
        Send a message to the vLLM server and get a response.
        
        Args:
            system_prompt: System prompt to set the context
            user_prompt: User prompt/question
            temperature: Temperature override for this call
            json_format: Whether to request JSON format response
            
        Returns:
            Tuple of (response_text, is_json_valid)
        """
        if temperature is None:
            temperature = self.temperature
            
        retry_count = 0
        max_retries = 3
        
        while retry_count < max_retries:
            try:
                # For vLLM, we use the completions endpoint with combined prompt
                combined_prompt = f"{system_prompt}\n{user_prompt}"
                
                response = self.client.completions.create(
                    model=self.model_name,
                    prompt=combined_prompt,
                    temperature=temperature,
                    max_tokens=self.max_tokens,
                    stop=self.stop_tokens,
                    stream=False
                )
                
                message = response.choices[0].text
                
                if not self._validate_response(message):
                    self.logger.warning(f"Invalid response received: {message}")
                    retry_count += 1
                    continue
                
                # Extract JSON if requested
                if json_format:
                    return self._extract_json(message)
                
                return message, False
                
            except RateLimitError as e:
                self.logger.warning(f"Rate limit hit: {e}")
                self._handle_rate_limit(retry_count, max_retries)
                retry_count += 1
                
            except Exception as e:
                self.logger.error(f"Error calling vLLM API: {e}")
                retry_count += 1
                if retry_count >= max_retries:
                    raise
                sleep(1)
        
        raise Exception(f"Failed to get response after {max_retries} retries")
    
    def check_health(self) -> bool:
        """Check if the vLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    @classmethod
    def get_available_models(cls):
        """Get list of popular models that work with vLLM."""
        return [
            "meta-llama/Llama-3.1-8B-Instruct",
            "meta-llama/Llama-3.1-70B-Instruct", 
            "meta-llama/Llama-3.1-405B-Instruct",
            "google/gemma-3-27b-it",
            "microsoft/DialoGPT-large",
            "mistralai/Mistral-7B-Instruct-v0.3"
        ]


class OllamaModel(BaseLLMModel):
    """Ollama model implementation for local Ollama deployments."""
    
    def __init__(self, model_name: str = "llama3.1:8b", 
                 base_url: str = "http://localhost:11434",
                 max_tokens: int = 1000, temperature: float = 0.7):
        """
        Initialize the Ollama model.
        
        Args:
            model_name: Name of the Ollama model to use
            base_url: Base URL for the Ollama server
            max_tokens: Maximum number of tokens to generate
            temperature: Temperature for sampling
        """
        super().__init__(model_name, max_tokens, temperature)
        
        self.base_url = base_url
        
        # Import ollama here to avoid dependency issues
        try:
            import ollama
            self.client = ollama.Client(host=base_url)
        except ImportError:
            raise ImportError("Please install ollama: pip install ollama")
        
    def send_msg(self, system_prompt: str, user_prompt: str, 
                 temperature: Optional[float] = None, 
                 json_format: bool = False) -> Tuple[str, bool]:
        """
        Send a message to the Ollama server and get a response.
        
        Args:
            system_prompt: System prompt to set the context
            user_prompt: User prompt/question
            temperature: Temperature override for this call
            json_format: Whether to request JSON format response
            
        Returns:
            Tuple of (response_text, is_json_valid)
        """
        if temperature is None:
            temperature = self.temperature
            
        retry_count = 0
        max_retries = 3
        
        while retry_count < max_retries:
            try:
                response = self.client.chat(
                    model=self.model_name,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    options={
                        "temperature": temperature,
                        "num_predict": self.max_tokens
                    }
                )
                
                message = response['message']['content']
                
                if not self._validate_response(message):
                    self.logger.warning(f"Invalid response received: {message}")
                    retry_count += 1
                    continue
                
                # Extract JSON if requested
                if json_format:
                    return self._extract_json(message)
                
                return message, False
                
            except Exception as e:
                self.logger.error(f"Error calling Ollama API: {e}")
                retry_count += 1
                if retry_count >= max_retries:
                    raise
                sleep(1)
        
        raise Exception(f"Failed to get response after {max_retries} retries")
    
    @classmethod
    def get_available_models(cls):
        """Get list of popular models that work with Ollama."""
        return [
            "llama3.1:8b",
            "llama3.1:70b", 
            "llama3.1:405b",
            "gemma3:27b",
            "mistral:7b",
            "codellama:7b"
        ] 