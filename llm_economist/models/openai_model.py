"""
OpenAI model implementation for the LLM Economist framework.
"""

from typing import Tuple, Optional
import os
try:
    from openai import OpenAI, RateLimitError
except ImportError:  # pragma: no cover - optional dependency
    OpenAI = None

    class RateLimitError(Exception):
        pass
from time import sleep
from .base import BaseLLMModel


class OpenAIModel(BaseLLMModel):
    """OpenAI model implementation using the OpenAI API."""
    
    def __init__(self, model_name: str = "gpt-4o-mini", api_key: Optional[str] = None,
                 max_tokens: int = 1000, temperature: float = 0.7):
        """
        Initialize the OpenAI model.
        
        Args:
            model_name: Name of the OpenAI model to use
            api_key: OpenAI API key (if None, will look for OPENAI_API_KEY env var)
            max_tokens: Maximum number of tokens to generate
            temperature: Temperature for sampling
        """
        super().__init__(model_name, max_tokens, temperature)
        
        # Get API key from parameter or environment
        if api_key is None:
            api_key = os.getenv('OPENAI_API_KEY') or os.getenv('ECON_OPENAI')

        if OpenAI is None:
            raise ImportError("openai package is required to use OpenAIModel")

        if not api_key:
            raise ValueError("OpenAI API key not found. Set OPENAI_API_KEY environment variable or pass api_key parameter.")
        
        self.client = OpenAI(api_key=api_key)
        
    def send_msg(self, system_prompt: str, user_prompt: str, 
                 temperature: Optional[float] = None, 
                 json_format: bool = False) -> Tuple[str, bool]:
        """
        Send a message to the OpenAI API and get a response.
        
        Args:
            system_prompt: System prompt to set the context
            user_prompt: User prompt/question
            temperature: Temperature override for this call
            json_format: Whether to request JSON format response
            
        Returns:
            Tuple of (response_text, is_json_valid)
        """
        if temperature is None:
            temperature = self.temperature
            
        retry_count = 0
        max_retries = 3
        
        while retry_count < max_retries:
            try:
                # Prepare the request
                request_params = {
                    "model": self.model_name,
                    "messages": [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    "temperature": temperature,
                    "max_tokens": self.max_tokens
                }
                
                # Add JSON format if requested
                if json_format:
                    request_params["response_format"] = {"type": "json_object"}
                
                # Make the API call
                response = self.client.chat.completions.create(**request_params)
                
                message = response.choices[0].message.content

                if not self._validate_response(message):
                    self.logger.warning(f"Invalid response received: {message}")
                    retry_count += 1
                    continue

                if json_format:
                    parsed_response, is_json_valid = self._extract_json(message)
                    self._record_history(
                        system_prompt,
                        user_prompt,
                        message,
                        json_requested=True,
                        is_json_valid=is_json_valid,
                        parsed_response=parsed_response,
                    )
                    return parsed_response, is_json_valid

                self._record_history(
                    system_prompt,
                    user_prompt,
                    message,
                    json_requested=False,
                    is_json_valid=False,
                )
                return message, False
                
            except RateLimitError as e:
                self.logger.warning(f"Rate limit hit: {e}")
                self._handle_rate_limit(retry_count, max_retries)
                retry_count += 1
                
            except Exception as e:
                self.logger.error(f"Error calling OpenAI API: {e}")
                retry_count += 1
                if retry_count >= max_retries:
                    raise
                sleep(1)
        
        raise Exception(f"Failed to get response after {max_retries} retries")
    
    @classmethod
    def get_available_models(cls):
        """Get list of available OpenAI models."""
        return [
            "gpt-4o",
            "gpt-4o-mini", 
            "gpt-4-turbo",
            "gpt-4",
            "gpt-3.5-turbo"
        ] 
