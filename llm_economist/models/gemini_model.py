"""
Gemini model implementation for the LLM Economist framework.
"""

from typing import Tuple, Optional
import os
import json
from time import sleep
from .base import BaseLLMModel


class GeminiModel(BaseLLMModel):
    """Gemini model implementation using Google's Gemini API."""
    
    def __init__(self, model_name: str = "gemini-1.5-flash", 
                 api_key: Optional[str] = None,
                 max_tokens: int = 1000, temperature: float = 0.7):
        """
        Initialize the Gemini model.
        
        Args:
            model_name: Name of the Gemini model to use
            api_key: Google API key (if None, will look for GOOGLE_API_KEY env var)
            max_tokens: Maximum number of tokens to generate
            temperature: Temperature for sampling
        """
        super().__init__(model_name, max_tokens, temperature)
        
        # Get API key from parameter or environment
        if api_key is None:
            api_key = os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')
        
        if not api_key:
            raise ValueError("Google API key not found. Set GOOGLE_API_KEY environment variable or pass api_key parameter.")
        
        self.api_key = api_key
        
        # Import Google AI SDK
        try:
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            self.client = genai
        except ImportError:
            raise ImportError("Please install Google AI SDK: pip install google-generativeai")
        
        # Initialize the model
        self.model = self.client.GenerativeModel(model_name)
        
    def send_msg(self, system_prompt: str, user_prompt: str, 
                 temperature: Optional[float] = None, 
                 json_format: bool = False) -> Tuple[str, bool]:
        """
        Send a message to the Gemini API and get a response.
        
        Args:
            system_prompt: System prompt to set the context
            user_prompt: User prompt/question
            temperature: Temperature override for this call
            json_format: Whether to request JSON format response
            
        Returns:
            Tuple of (response_text, is_json_valid)
        """
        if temperature is None:
            temperature = self.temperature
            
        retry_count = 0
        max_retries = 3
        
        while retry_count < max_retries:
            try:
                # Combine system and user prompts
                combined_prompt = f"{system_prompt}\n\n{user_prompt}"
                
                # Add JSON format instruction if requested
                if json_format:
                    combined_prompt += "\n\nPlease respond in valid JSON format."
                
                # Configure generation parameters
                generation_config = self.client.types.GenerationConfig(
                    temperature=temperature,
                    max_output_tokens=self.max_tokens,
                    candidate_count=1
                )
                
                # Generate response
                response = self.model.generate_content(
                    combined_prompt,
                    generation_config=generation_config
                )
                
                message = response.text

                if not self._validate_response(message):
                    self.logger.warning(f"Invalid response received: {message}")
                    retry_count += 1
                    continue

                if json_format:
                    parsed_response, is_json_valid = self._extract_json(message)
                    self._record_history(
                        system_prompt,
                        user_prompt,
                        message,
                        json_requested=True,
                        is_json_valid=is_json_valid,
                        parsed_response=parsed_response,
                    )
                    return parsed_response, is_json_valid

                self._record_history(
                    system_prompt,
                    user_prompt,
                    message,
                    json_requested=False,
                    is_json_valid=False,
                )
                return message, False
                
            except Exception as e:
                if "quota" in str(e).lower() or "rate" in str(e).lower():
                    self.logger.warning(f"Rate limit or quota exceeded: {e}")
                    self._handle_rate_limit(retry_count, max_retries)
                    retry_count += 1
                else:
                    self.logger.error(f"Error calling Gemini API: {e}")
                    retry_count += 1
                    if retry_count >= max_retries:
                        raise
                    sleep(1)
        
        raise Exception(f"Failed to get response after {max_retries} retries")
    
    @classmethod
    def get_available_models(cls):
        """Get list of available Gemini models."""
        return [
            "gemini-1.5-pro",
            "gemini-1.5-flash",
            "gemini-1.0-pro",
            "gemini-1.0-pro-latest"
        ]
    
    def list_models(self):
        """List all available models dynamically."""
        try:
            models = list(self.client.list_models())
            return [model.name for model in models if 'generateContent' in model.supported_generation_methods]
        except Exception as e:
            self.logger.error(f"Error listing models: {e}")
            return self.get_available_models()


class GeminiModelViaOpenRouter(BaseLLMModel):
    """Gemini model implementation using OpenRouter as a proxy."""
    
    def __init__(self, model_name: str = "google/gemini-flash-1.5", 
                 api_key: Optional[str] = None,
                 max_tokens: int = 1000, temperature: float = 0.7):
        """
        Initialize the Gemini model via OpenRouter.
        
        Args:
            model_name: Name of the Gemini model on OpenRouter
            api_key: OpenRouter API key (if None, will look for OPENROUTER_API_KEY env var)
            max_tokens: Maximum number of tokens to generate
            temperature: Temperature for sampling
        """
        super().__init__(model_name, max_tokens, temperature)
        
        # Import OpenRouter model
        from .openrouter_model import OpenRouterModel
        
        self.openrouter_client = OpenRouterModel(
            model_name=model_name,
            api_key=api_key,
            max_tokens=max_tokens,
            temperature=temperature
        )

        # Share history with the proxied client so persistence helpers stay in sync
        self.history = self.openrouter_client.history
        
    def send_msg(self, system_prompt: str, user_prompt: str, 
                 temperature: Optional[float] = None, 
                 json_format: bool = False) -> Tuple[str, bool]:
        """
        Send a message to the Gemini API via OpenRouter and get a response.
        
        Args:
            system_prompt: System prompt to set the context
            user_prompt: User prompt/question
            temperature: Temperature override for this call
            json_format: Whether to request JSON format response
            
        Returns:
            Tuple of (response_text, is_json_valid)
        """
        result = self.openrouter_client.send_msg(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=temperature,
            json_format=json_format
        )
        # Ensure local history reference stays aligned with the proxied model
        self.history = self.openrouter_client.history
        return result
    
    @classmethod
    def get_available_models(cls):
        """Get list of available Gemini models on OpenRouter."""
        return [
            "google/gemini-pro-1.5",
            "google/gemini-flash-1.5",
            "google/gemini-pro"
        ] 
