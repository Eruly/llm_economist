"""
Main entry point for the LLM Economist framework.
"""

import argparse
import logging
import os
import sys
import concurrent.futures
import torch.multiprocessing as mp
import trackio as wandb
import random
import numpy as np
import time
from .utils.common import distribute_agents, count_votes, rGB2, GEN_ROLE_MESSAGES
from .agents.worker import Worker, FixedWorker, distribute_personas
from .agents.llm_agent import LLMAgent, TestAgent
from .agents.planner import TaxPlanner, FixedTaxPlanner


def setup_logging(args):
    """Setup logging configuration."""
    if not os.path.exists(args.log_dir):
        os.makedirs(args.log_dir)
    
    log_filename = f'{args.log_dir}/{args.name if args.name else "simulation"}.log'
    logging.basicConfig(
        filename=log_filename,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )


def run_simulation(args):
    """Run the main simulation."""
    logger = logging.getLogger('main')
    
    # Test LLM connectivity
    if args.worker_type == 'LLM' or args.planner_type == 'LLM':
        try:
            TestAgent(args.llm, args.port, args)
            logger.info(f"Successfully connected to LLM: {args.llm}")
        except Exception as e:
            logger.error(f"Failed to connect to LLM: {e}")
            if args.worker_type == 'LLM' or args.planner_type == 'LLM':
                sys.exit(1)
    
    # Initialize skill distribution
    if args.agent_mix == 'uniform':
        skills = [-1] * args.num_agents  # maps to uniform distribution in worker.py
    elif args.agent_mix == 'us_income':
        # U.S. incomes to skill level (income level is at 40 hours per week)
        skills = [float(x / 40) for x in rGB2(args.num_agents)] 
        print(skills)
        logger.info(f"Skills sampled from GB2 Distribution: {skills}")
    else:
        raise ValueError(f'Unknown agent mix: {args.agent_mix}')
    
    # Initialize agents
    agents = []
    personas = []
    
    if args.scenario == 'rational':
        personas = ['default' for i in range(args.num_agents)]
        utility_types = ['egotistical' for i in range(args.num_agents)]
    elif args.scenario in ('bounded', 'democratic'):
        # Generate personas
        persona_data = distribute_personas(args.num_agents, args.llm, args.port, args.service)
        global GEN_ROLE_MESSAGES
        GEN_ROLE_MESSAGES.clear()
        GEN_ROLE_MESSAGES.update(persona_data)
        personas = list(GEN_ROLE_MESSAGES.keys())
        
        assert (args.percent_ego + args.percent_alt + args.percent_adv) == 100
        utility_types = distribute_agents(args.num_agents, [args.percent_ego, args.percent_alt, args.percent_adv])
        print('utility_types', utility_types)
        logger.info(f"Utility Types: {utility_types}")
    
    # Create worker agents
    for i in range(args.num_agents):
        name = f"worker_{i}"
        if args.worker_type == 'LLM' or (args.worker_type == 'ONE_LLM' and i == 0):
            agent = Worker(args.llm, 
                           args.port, 
                           name, 
                           utility_type=utility_types[i],
                           history_len=args.history_len, 
                           prompt_algo=args.prompt_algo, 
                           max_timesteps=args.max_timesteps, 
                           two_timescale=args.two_timescale,
                           role=personas[i], 
                           scenario=args.scenario, 
                           num_agents=args.num_agents,
                           args=args,
                           skill=skills[i],
                           )
        else:
            agent = FixedWorker(name, history_len=args.history_len, labor=np.random.randint(40, 61), args=args)
        agents.append(agent)
    
    # Initialize tax planner
    if args.planner_type == 'LLM':
        planner_history = args.history_len
        if args.num_agents > 20:
            planner_history = args.history_len//(args.num_agents) * 20
        
        tax_planner = TaxPlanner(args.llm, args.port, 'Joe', 
                                 history_len=planner_history, prompt_algo=args.prompt_algo, 
                                 max_timesteps=args.max_timesteps, num_agents=args.num_agents, args=args)
    elif args.planner_type in ['US_FED', 'SAEZ', 'SAEZ_FLAT', 'SAEZ_THREE', 'UNIFORM']:
        tax_planner = FixedTaxPlanner('Joe', args.planner_type, history_len=args.history_len, skills=skills, args=args)
    tax_rates = tax_planner.tax_rates
    
    # Initialize wandb logging
    if args.wandb:
        experiment_name = generate_experiment_name(args)
        wandb.init(
            project="llm-economist",
            name=experiment_name,
            config=vars(args)
        )
    
    start_time = time.time()

    llm_entities = [
        agent for agent in agents
        if isinstance(agent, LLMAgent) and getattr(agent, 'llm', None) is not None
    ]
    if isinstance(tax_planner, LLMAgent) and getattr(tax_planner, 'llm', None) is not None:
        llm_entities.append(tax_planner)

    def save_histories_if_due(timestep: int, *, force: bool = False) -> None:
        for entity in llm_entities:
            entity.maybe_save_history(timestep, force=force)

    # Main simulation loop
    for k in range(args.max_timesteps):
        logger.info(f"TIMESTEP {k}")
        print(f"TIMESTEP {k}")
        
        wandb_logger = {}
        
        # Get new tax rates
        workers_stats = [(agent.z, agent.utility) for agent in agents]
        # do not set tax rates during warmup period
        if k % args.two_timescale == 0 and args.planner_type == 'LLM' and k >= args.warmup:
            if args.scenario == 'democratic':
                # Use ThreadPoolExecutor for parallel execution of agent actions
                with concurrent.futures.ThreadPoolExecutor(max_workers=args.num_agents) as executor:
                    if args.platforms:
                        max_retries = 10
                        retry_count = 0
                        candidates = []
                        while not candidates and retry_count < max_retries:
                            futures0 = [executor.submit(agent.act_pre_vote, k) for agent in agents]
                            concurrent.futures.wait(futures0)
                            candidates = [(agent.name.split("_")[-1], agent.platform) for agent in agents if agent.platform != None]
                            retry_count += 1  # Prevent infinite loop
                        logger.info(f"Candidates: {candidates}")
                        print("Candidates: ", candidates)
                        if not candidates:
                            print("No candidates, so all agent.vote were not updated, and current leader stays in power")
                            logger.info(f"No candidates, so all agent.vote were not updated, and current leader stays in power")
                        else:
                            futures = [executor.submit(agent.act_vote_platform, candidates, k) for agent in agents]
                            concurrent.futures.wait(futures)
                    else:
                        futures = [executor.submit(agent.act_vote, k) for agent in agents]
                        concurrent.futures.wait(futures)
                votes_list = [agent.vote for agent in agents]
                print("Votes: ", votes_list)
                leader_agent = count_votes(votes_list)
                leader = agents[leader_agent] # unnecessary
                wandb_logger[f"leader"] = leader_agent
                print("leader: ", leader_agent)
                if args.platforms:
                    for agent in agents:
                        agent.update_leader(k, leader_agent, candidates)
                    tax_planner.update_leader(k, leader_agent, candidates)
                else:
                    for agent in agents:
                        agent.update_leader(k, leader_agent)
                    tax_planner.update_leader(k, leader_agent)
                # get tax rate
                # get message from planner for agent features only
                planner_state = tax_planner.get_state(k, workers_stats, True)
                # find tax rate
                tax_delta = agents[leader_agent].act_plan(k, planner_state)[0]
                print("act_leader: ", tax_delta)
                for agent in agents:
                    agent.update_leader_action(k, tax_delta)
                tax_planner.update_leader_action(k, tax_delta) # not necessary potentially with act_log_only, which format preferred?
                tax_rates = tax_planner.act_log_only(tax_delta, k)
                for agent in agents:
                    agent.tax_rates = tax_rates
            else:
                tax_rates = tax_planner.act(k, workers_stats)
                print("act: ", tax_rates)
        elif args.planner_type == 'LLM':
            tax_planner.add_obs_msg(k, workers_stats)
            tax_planner.add_act_msg(k, tax_rates=tax_rates)

        planner_state = None
        if args.percent_ego < 100:
            planner_state = tax_planner.get_state(k, workers_stats, False) # for adversarial and altruistic agents
            
        if args.use_multithreading:
            # Use ThreadPoolExecutor for parallel execution of agent actions
            with concurrent.futures.ThreadPoolExecutor(max_workers=args.num_agents) as executor:
                futures = [executor.submit(agent.act, k, tax_rates, planner_state) for agent in agents]
                concurrent.futures.wait(futures)
        else:
            for i in range(args.num_agents):
                agents[i].act(k, tax_rates, planner_state)

        pre_tax_incomes = [agents[i].z for i in range(args.num_agents)]
        
        # Calculate taxes
        post_tax_incomes, total_tax = tax_planner.apply_taxes(tax_rates, pre_tax_incomes)
        tax_indv = np.array(pre_tax_incomes) - np.array(post_tax_incomes)
        tax_rebate_avg = total_tax / args.num_agents
        
        # Update agent utilities
        for i, agent in enumerate(agents):
            agent.tax_paid = tax_indv[i]
        if args.scenario == 'bounded' and args.use_multithreading:
            with concurrent.futures.ThreadPoolExecutor(max_workers=args.num_agents) as executor:
                futures = [executor.submit(agents[i].update_utility, k, post_tax_incomes[i], tax_rebate_avg, tax_planner.swf) for i in range(args.num_agents)]
                concurrent.futures.wait(futures)
        else:
            for i, agent in enumerate(agents):
                agent.update_utility(k, post_tax_incomes[i], tax_rebate_avg, tax_planner.swf)
        for i, agent in enumerate(agents):
            agent.log_stats(k, wandb_logger, debug=args.debug)
        
        # Log tax planner stats
        # use isoelastic utility by default for altruistic/adversarial planner swf
        u = [agents[i].utility if agents[i].utility_type == 'egotistical' else agents[i].compute_isoelastic_utility(post_tax_incomes[i], tax_rebate_avg) for i in range(args.num_agents)]
        tax_planner.log_stats(k, wandb_logger, z=pre_tax_incomes, u=u, debug=args.debug)
        
        if args.wandb:
            wandb.log(wandb_logger)
            
        end_time = time.time()
        iteration_time = end_time - start_time
        total_actions = (k + 1) * args.num_agents  # Total actions performed so far

        fps = (k + 1) / iteration_time
        aps = total_actions / iteration_time

        logger.info(f"Time for iteration 0-{k+1}: {iteration_time:.5f} seconds")
        logger.info(f"FPS: {fps:.2f}")
        logger.info(f"APS: {aps:.2f}")

        remaining_time = (args.max_timesteps - k - 1) * iteration_time / (k + 1)
        logger.info(f"Time remaining {k+2}-{args.max_timesteps}: {remaining_time:.5f} seconds")

        save_histories_if_due(k)

    logger.info("Simulation completed successfully!")

    save_histories_if_due(args.max_timesteps - 1, force=True)
    
    if args.wandb:
        wandb.finish()


def generate_experiment_name(args):
    """Generate a descriptive experiment name."""
    # Start with scenario and number of agents as base
    name_parts = [f"{args.scenario}"]
    name_parts.append(f"a{args.num_agents}")
    
    # Add agent composition if not all egotistical
    if args.percent_ego != 100:
        name_parts.append(f"mix_e{args.percent_ego}_a{args.percent_alt}_d{args.percent_adv}")
    
    # Add worker and planner types
    name_parts.append(f"w-{args.worker_type}")
    name_parts.append(f"p-{args.planner_type}")
    
    # Add LLM model (shortened)
    llm_name = args.llm.replace("llama3:", "l3-").replace("gpt-", "g").replace("-mini-2024-07-18", "m")
    name_parts.append(f"llm-{llm_name}")
    
    # Add prompting algorithm
    name_parts.append(f"prompt-{args.prompt_algo}")
    
    # Add timescale and history length
    name_parts.append(f"ts{args.two_timescale}")
    name_parts.append(f"hist{args.history_len}")
    
    # Add max timesteps
    name_parts.append(f"steps{args.max_timesteps}")
    
    # Add bracket setting
    name_parts.append(f"bracket-{args.bracket_setting}")
    
    # Add voting indicator if using platforms
    if args.platforms:
        name_parts.append("voting")
    
    # Join all parts with underscores
    return "_".join(name_parts)


def create_argument_parser():
    """Create and return the argument parser."""
    parser = argparse.ArgumentParser(description='AI Economist Simulation')
    parser.add_argument('--num-agents', type=int, default=5, help='Number of agents in the simulation')
    parser.add_argument('--worker-type', default='LLM', choices=['LLM', 'FIXED', 'ONE_LLM'], help='Type of worker agents')
    parser.add_argument('--planner-type', default='LLM', choices=['LLM', 'US_FED', 'SAEZ', 'SAEZ_THREE', 'SAEZ_FLAT', 'UNIFORM'], help='Type of tax planner')
    parser.add_argument('--max-timesteps', type=int, default=1000, help='Maximum number of timesteps for the simulation')
    parser.add_argument('--history-len', type=int, default=50, help='Length of history to consider')
    parser.add_argument('--history-jsonl-load', type=str, default=None,
                        help='Path (file or directory) to load LLM history JSONL data from')
    parser.add_argument('--history-jsonl-step', type=int, default=None,
                        help='Restore history up to the specified step when loading')
    parser.add_argument('--history-jsonl-save', type=str, default=None,
                        help='Path (file or directory) to save LLM history JSONL data to')
    parser.add_argument('--history-save-interval', type=int, default=0,
                        help='Save history every N timesteps; 0 disables periodic saves')
    parser.add_argument('--two-timescale', type=int, default=25, help='Interval for two-timescale updates')
    parser.add_argument('--debug', type=bool, default=True, help='Enable debug mode') 
    parser.add_argument('--llm', default='llama3:8b', type=str, help='Language model to use')
    parser.add_argument('--prompt-algo', default='io', choices=['io', 'cot'], help='Prompting algorithm to use')
    parser.add_argument('--scenario', default='rational', choices=['rational', 'bounded', 'democratic'], help='Scenario')
    parser.add_argument('--percent-ego', type=int, default=100)
    parser.add_argument('--percent-alt', type=int, default=0)
    parser.add_argument('--percent-adv', type=int, default=0)
    parser.add_argument('--port', type=int, default=8009)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--agent-mix', default='us_income', choices=['uniform', 'us_income'], help='Distribution of agents\' skill level')
    parser.add_argument('--platforms', action="store_true", help='Agents choose to run with a platform in election')
    parser.add_argument('--name', type=str, default='', help='Experiment name')
    parser.add_argument('--log-dir', type=str, default='logs', help='Directory for log files')
    parser.add_argument('--bracket-setting', default='three', choices=['flat', 'three', 'US_FED'])
    parser.add_argument('--service', default='vllm', choices=['vllm', 'ollama'])
    parser.add_argument('--use-multithreading', action='store_true')
    parser.add_argument('--warmup', default=0, type=int)
    parser.add_argument('--elasticity', nargs='+', type=float, default=[0.4], 
                    help='Elasticity values for tax brackets')
    parser.add_argument('--wandb', action='store_true', help='Enable wandb logging')
    parser.add_argument('--timeout', type=int, default=30, help='Timeout for LLM calls')
    
    return parser


def main():
    """Main entry point."""
    parser = create_argument_parser()
    args = parser.parse_args()

    if not args.name:
        args.name = generate_experiment_name(args)

    # Setup logging
    setup_logging(args)
    
    # Create a logger in the main process
    logger = logging.getLogger('main')
    pid = os.getpid()

    logger.info(f"Main process started: {args.name}")
    logger.info(f'PID: {pid}')
    logger.info(args)
    
    # Set random seeds
    np.random.seed(args.seed)
    random.seed(args.seed)
    
    start_time = time.time()
    
    run_simulation(args)
    
    end_time = time.time()
    print(f"Total simulation time: {end_time - start_time:.2f} seconds")


if __name__ == '__main__':
    mp.set_start_method('spawn', force=True)
    main() 
